# DeerFlow 报告评估算法详解

## 文档概述

本文档详细说明 DeerFlow 深度研究框架的报告质量评估算法，包括评分机制、权重分配和等级评定标准。

**文档版本**: 1.0
**最后更新**: 2026-02-10
**适用版本**: DeerFlow main 分支

---

## 目录

- [评估算法概述](#评估算法概述)
- [自动化指标评估](#自动化指标评估)
- [LLM 评判机制](#llm-评判机制)
- [最终分数计算](#最终分数计算)
- [等级评定标准](#等级评定标准)
- [评估示例](#评估示例)
- [使用方法](#使用方法)

---

## 评估算法概述

DeerFlow 使用**双重评估机制**来全面评估研究报告的质量：

```
最终分数 = 自动化指标分数 × 40% + LLM 评判分数 × 60%
```

### 设计理念

1. **平衡客观与主观**
   - 自动化指标：客观、快速、可重复
   - LLM 评判：主观、细致、上下文感知

2. **权重倾向 LLM**
   - LLM 占 60%，因为它能理解内容质量和深度
   - 指标占 40%，提供基础质量保证

3. **容错机制**
   - 如果 LLM 评估失败，自动降级到纯指标评估
   - 确保系统始终能给出评分

### 核心文件

- `src/eval/evaluator.py` - 评估器主逻辑
- `src/eval/metrics.py` - 自动化指标计算
- `src/eval/llm_judge.py` - LLM 评判实现

---

## 自动化指标评估

### 评分权重分配

自动化指标评估总分为 10 分，由以下 5 个维度组成：

| 评估维度 | 权重 | 目标值 | 说明 |
|---------|------|--------|------|
| 章节覆盖率 | 30% | 100% | 是否包含所有必需章节 |
| 引用质量 | 25% | 10 个 | 引用数量和质量 |
| 字数合规性 | 20% | 根据风格 | 是否符合字数范围要求 |
| 来源多样性 | 15% | 5 个 | 独立信息源数量 |
| 图片包含 | 10% | 3 张 | 图片/图表数量 |

**代码位置**: `src/eval/evaluator.py:88-126`

---

### 1. 章节覆盖率（30%）

**计算公式**：
```python
section_score = (已包含章节数 / 必需章节总数) * 10
最终贡献 = section_score * 0.30
```

**评分示例**：
- 包含所有必需章节（5/5）：10 分 → 贡献 3.0 分
- 缺少 1 个章节（4/5）：8 分 → 贡献 2.4 分
- 缺少 2 个章节（3/5）：6 分 → 贡献 1.8 分

---

### 2. 引用质量（25%）

**计算公式**：
```python
citation_score = min(引用数量 / 10, 1.0) * 10
最终贡献 = citation_score * 0.25
```

**评分规则**：
- 引用数 ≥ 10：满分 10 分
- 引用数 < 10：按比例计分

**评分示例**：
- 10 个引用：10 分 → 贡献 2.5 分
- 5 个引用：5 分 → 贡献 1.25 分
- 15 个引用：10 分 → 贡献 2.5 分（不额外加分）
- 0 个引用：0 分 → 贡献 0 分

---

### 3. 字数合规性（20%）

**计算公式**：
```python
if 最小字数 <= 实际字数 <= 最大字数:
    word_score = 10.0  # 完美符合
elif 实际字数 < 最小字数:
    word_score = (实际字数 / 最小字数) * 8  # 字数不足，最多扣20%
else:
    超出比例 = 实际字数 / 最大字数
    word_score = max(10 - (超出比例 - 1) * 5, 5)  # 字数过多，最低5分

最终贡献 = word_score * 0.20
```

**不同报告风格的字数要求**：

| 报告风格 | 最小字数 | 最大字数 | 说明 |
|---------|---------|---------|------|
| academic | 5,000 | 15,000 | 学术研究报告 |
| strategic_investment | 10,000 | 20,000 | 战略投资分析 |
| popular_science | 3,000 | 8,000 | 科普文章 |
| news | 1,000 | 3,000 | 新闻报道 |
| social_media | 500 | 1,500 | 社交媒体内容 |

**评分示例**（academic 风格，目标 5,000-15,000 字）：
- 8,000 字（在范围内）：10 分 → 贡献 2.0 分
- 3,000 字（不足 60%）：4.8 分 → 贡献 0.96 分
- 20,000 字（超出 33%）：6.67 分 → 贡献 1.33 分

---

### 4. 来源多样性（15%）

**计算公式**：
```python
diversity_score = min(独立来源数 / 5, 1.0) * 10
最终贡献 = diversity_score * 0.15
```

**评分规则**：
- 来源数 ≥ 5：满分 10 分
- 来源数 < 5：按比例计分

**评分示例**：
- 5 个来源：10 分 → 贡献 1.5 分
- 3 个来源：6 分 → 贡献 0.9 分
- 8 个来源：10 分 → 贡献 1.5 分

---

### 5. 图片包含（10%）

**计算公式**：
```python
image_score = min(图片数量 / 3, 1.0) * 10
最终贡献 = image_score * 0.10
```

**评分规则**：
- 图片数 ≥ 3：满分 10 分
- 图片数 < 3：按比例计分

**评分示例**：
- 3 张图片：10 分 → 贡献 1.0 分
- 1 张图片：3.33 分 → 贡献 0.33 分
- 5 张图片：10 分 → 贡献 1.0 分

---

## LLM 评判机制

### 评估维度

LLM 从以下 6 个维度对报告进行评分，每项 0-10 分：

| 维度 | 权重 | 说明 |
|------|------|------|
| Relevance（相关性） | 20% | 报告是否回答了研究问题，内容是否切题 |
| Depth（深度） | 20% | 分析是否深入，是否提供详细论述 |
| Accuracy（准确性） | 20% | 信息是否准确，是否有事实错误 |
| Structure（结构） | 15% | 组织是否清晰，逻辑是否连贯 |
| Clarity（清晰度） | 15% | 表达是否清楚，是否易于理解 |
| Completeness（完整性） | 10% | 是否覆盖所有重要方面，是否有遗漏 |

**代码位置**: `src/eval/llm_judge.py`

---

### LLM 加权分数计算

**计算公式**：
```python
LLM_加权分数 = (
    相关性 × 0.20 +
    深度 × 0.20 +
    准确性 × 0.20 +
    结构 × 0.15 +
    清晰度 × 0.15 +
    完整性 × 0.10
)
```

**评分示例**：
- 相关性：9/10
- 深度：8/10
- 准确性：9/10
- 结构：8/10
- 清晰度：9/10
- 完整性：7/10

**计算过程**：
```
= 9 × 0.20 + 8 × 0.20 + 9 × 0.20 + 8 × 0.15 + 9 × 0.15 + 7 × 0.10
= 1.8 + 1.6 + 1.8 + 1.2 + 1.35 + 0.7
= 8.45
```

---

### LLM 评估输出

除了数值评分，LLM 还会提供定性反馈：

**1. 优点（Strengths）**
- 列出报告的 3-5 个突出优点
- 例如："数据分析详实"、"逻辑结构清晰"

**2. 缺点（Weaknesses）**
- 列出 3-5 个需要改进的方面
- 例如："缺少具体案例"、"结论部分过于简短"

---

## 最终分数计算

### 综合评分公式

**代码位置**: `src/eval/evaluator.py:197-200`

```python
if LLM评估成功:
    最终分数 = 自动化指标分数 × 0.4 + LLM加权分数 × 0.6
else:
    最终分数 = 自动化指标分数  # 降级到纯指标评估
```

### 计算示例

假设：
- 自动化指标分数：8.5
- LLM 加权分数：9.0

**计算过程**：
```
最终分数 = 8.5 × 0.4 + 9.0 × 0.6
        = 3.4 + 5.4
        = 8.8
```

---

## 等级评定标准

### 等级对照表

**代码位置**: `src/eval/evaluator.py:41-64`

| 分数范围 | 等级 | 说明 |
|---------|------|------|
| 9.0 - 10.0 | A+ | 优秀，几乎完美的报告 |
| 8.5 - 8.9 | A | 优秀，高质量报告 |
| 8.0 - 8.4 | A- | 优秀，质量良好 |
| 7.5 - 7.9 | B+ | 良好，略有不足 |
| 7.0 - 7.4 | B | 良好，符合基本要求 |
| 6.5 - 6.9 | B- | 良好，有改进空间 |
| 6.0 - 6.4 | C+ | 及格，需要改进 |
| 5.5 - 5.9 | C | 及格，存在明显问题 |
| 5.0 - 5.4 | C- | 及格，问题较多 |
| 4.0 - 4.9 | D | 不及格，质量较差 |
| < 4.0 | F | 不及格，质量很差 |

---

## 评估示例

### 完整评估案例

假设一份 **academic** 风格的研究报告：

#### 自动化指标数据

| 指标 | 实际值 | 目标值 | 单项得分 | 权重 | 贡献分数 |
|------|--------|--------|---------|------|---------|
| 章节覆盖 | 5/5 | 5/5 | 10.0 | 30% | 3.00 |
| 引用数量 | 12 个 | 10 个 | 10.0 | 25% | 2.50 |
| 字数 | 8,000 | 5,000-15,000 | 10.0 | 20% | 2.00 |
| 独立来源 | 6 个 | 5 个 | 10.0 | 15% | 1.50 |
| 图片数量 | 2 张 | 3 张 | 6.67 | 10% | 0.67 |

**自动化指标总分**：3.00 + 2.50 + 2.00 + 1.50 + 0.67 = **9.67**

#### LLM 评判数据

| 维度 | 得分 | 权重 | 贡献分数 |
|------|------|------|---------|
| 相关性 | 9/10 | 20% | 1.80 |
| 深度 | 8/10 | 20% | 1.60 |
| 准确性 | 9/10 | 20% | 1.80 |
| 结构 | 8/10 | 15% | 1.20 |
| 清晰度 | 9/10 | 15% | 1.35 |
| 完整性 | 7/10 | 10% | 0.70 |

**LLM 加权分数**：1.80 + 1.60 + 1.80 + 1.20 + 1.35 + 0.70 = **8.45**

#### 最终分数计算

```
最终分数 = 9.67 × 0.4 + 8.45 × 0.6
        = 3.87 + 5.07
        = 8.94
```

**等级评定**：**A**（8.5-8.9 分）

#### 评估报告示例

```
Report Grade: A (8.94/10)

**Automated Metrics:**
- Word Count: 8000
- Citations: 12
- Unique Sources: 6
- Images: 2
- Section Coverage: 100%

**LLM Evaluation:**
- Relevance: 9/10
- Depth: 8/10
- Accuracy: 9/10
- Structure: 8/10
- Clarity: 9/10
- Completeness: 7/10

**Strengths:**
- 数据分析详实，引用充分
- 逻辑结构清晰，论证严谨
- 语言表达流畅，易于理解

**Areas for Improvement:**
- 可以增加更多图表辅助说明
- 完整性方面可以补充更多案例
```

---

## 使用方法

### 1. 完整评估（推荐）

使用自动化指标 + LLM 评判的完整评估：

```python
from src.eval.evaluator import ReportEvaluator
from src.llms.llm import get_llm_by_type

# 初始化评估器
llm = get_llm_by_type("basic")
evaluator = ReportEvaluator(llm=llm, use_llm=True)

# 异步评估
result = await evaluator.evaluate(
    report=report_text,
    query="研究主题",
    report_style="academic"
)

# 查看结果
print(f"等级: {result.grade}")
print(f"分数: {result.final_score}")
print(result.summary)
```

### 2. 同步评估

如果不在异步环境中：

```python
# 同步评估
result = evaluator.evaluate_sync(
    report=report_text,
    query="研究主题",
    report_style="academic"
)
```

### 3. 仅指标评估（快速模式）

如果只需要快速评估，不使用 LLM：

```python
# 仅使用自动化指标
result = evaluator.evaluate_metrics_only(
    report=report_text,
    report_style="academic"
)

print(f"等级: {result['grade']}")
print(f"分数: {result['score']}")
```

### 4. 禁用 LLM 评估

如果想完全禁用 LLM 评估以提高速度：

```python
# 初始化时禁用 LLM
evaluator = ReportEvaluator(use_llm=False)

# 评估时只使用自动化指标
result = await evaluator.evaluate(
    report=report_text,
    query="研究主题",
    report_style="academic"
)
```

---

## 配置说明

### 支持的报告风格

评估器支持以下报告风格，每种风格有不同的字数要求：

- `academic` - 学术研究报告（5,000-15,000 字）
- `strategic_investment` - 战略投资分析（10,000-20,000 字）
- `popular_science` - 科普文章（3,000-8,000 字）
- `news` - 新闻报道（1,000-3,000 字）
- `social_media` - 社交媒体内容（500-1,500 字）

### LLM 模型选择

建议使用以下模型进行 LLM 评判：

- **推荐**：GPT-4、Claude 3 Opus - 评判质量最高
- **平衡**：GPT-3.5-turbo、Claude 3 Sonnet - 速度和质量平衡
- **快速**：禁用 LLM，仅使用自动化指标

---

## 注意事项

### 1. 性能考虑

- **完整评估**：包含 LLM 调用，耗时约 5-10 秒
- **仅指标评估**：纯计算，耗时 < 1 秒
- 建议：开发测试时使用仅指标模式，生产环境使用完整评估

### 2. 容错机制

- 如果 LLM 评估失败（网络错误、超时等），系统会自动降级到纯指标评估
- 确保始终能得到评分结果

### 3. 评分偏差

- 自动化指标偏向量化指标（字数、引用数等）
- LLM 评判更关注内容质量和深度
- 两者结合可以平衡量化和质化评估

### 4. 自定义评分权重

如需调整评分权重，可以修改以下文件：

- 自动化指标权重：`src/eval/evaluator.py:88-126`
- LLM 维度权重：`src/eval/llm_judge.py`
- 最终分数权重：`src/eval/evaluator.py:197-200`

---

## 总结

### 评估算法优势

1. **双重机制**：结合客观指标和主观评判，全面评估报告质量
2. **灵活配置**：支持多种报告风格，可禁用 LLM 以提高速度
3. **容错设计**：LLM 失败时自动降级，确保始终有评分
4. **详细反馈**：提供数值评分、等级和改进建议

